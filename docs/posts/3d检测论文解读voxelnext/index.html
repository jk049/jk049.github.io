<!DOCTYPE html>
<html class="js no-touch  progressive-image  no-reduced-motion progressive" lang="en">
  <head>
    <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="icon" href="/img/favicon.ico">

    <meta name="keyword" content="">

    <title>3D检测论文解读——VoxelNext</title>

    <link rel="canonical" href="/posts/3d%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BBvoxelnext/">

    <link rel="stylesheet" href="/css/global.css">

    <link rel="stylesheet" href="/css/custom.css">

    <link rel="stylesheet" href="/css/search.css" />

    
    

    
    

</head>
  </head>
  <body class=" page-article   ">
    <header>
      <nav class="nav">
  <div class="nav-wrapper">
    <div class="nav-content-wrapper">
      <div class="nav-content">
        <a href="/ " class="nav-title">jk049&#39;s blog</a>
        <div class="nav-menu">
          <div class="nav-item-wrapper">
            <a href="/posts " class="nav-item-content">Articles</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/about" class="nav-item-content">About</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/index.xml" class="nav-item-content" target="_blank">RSS</a>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</nav>

<script>
  function toggleSearchModal(){
    const template = `
    <div class="modal-body">
      <div id="autocomplete" onclick="event.stopPropagation();"></div>
    </div>
    `
    const modal = document.querySelector("#modal-wrapper")
    if(!modal){
      const div = document.createElement("div")
      document.body.setAttribute("style","overflow: hidden;")
      div.setAttribute("id", "modal-wrapper")
      div.setAttribute("onclick", "toggleSearchModal()")
      div.innerHTML = template
      const script = document.createElement("script");script.setAttribute("src", "https://jk049.github.io/js/algolia.js")
      div.appendChild(script)
      document.body.append(div)
    } else {
      document.body.removeAttribute("style")
      document.body.removeChild(modal)
    }
  }
</script>
    </header>
    
  
  
  <main id="main" class="main">
      <section>
        <article class="article">
          
          <div class=" article-header ">
            <div class="category component">
              <div class="component-content">
                <div class="category-eyebrow">
                  <span class="category-eyebrow__category category_original">
                    
                      
                        next
                      
                    
                  </span>
                  <span class="category-eyebrow__date">May 22, 2023</span>
                </div>
              </div>
            </div>
            <div class="pagetitle component">
              <div class="component-content">
                <h1 class="hero-headline">3D检测论文解读——VoxelNext</h1>
              </div>
            </div>
            <div class="component  article-subhead ">
              <div class="component-content">全稀疏结构的voxel框架，想替代当前的SECOND结构，称为下一代voxel基线框架</div>
            </div>

            <div class="tagssheet component">
              <div class="component-content">
                
                  
                  <a href="/tags/next" class="tag">
                    next
                  </a>
                
                  
                  <a href="/tags/point" class="tag">
                    point
                  </a>
                
                  
                  <a href="/tags/3d" class="tag">
                    3D
                  </a>
                
                  
                  <a href="/tags/voxel" class="tag">
                    voxel
                  </a>
                
              </div>
            </div>
          </div>
          
          <div class="pagebody">
            
            
            
            
            
            
            
            
            
<div class="component-content pagebody component">
  <h1 id="一句话总结" class="pagebody-header">
    一句话总结
  </h1>
</div><p class="component-content component">这篇文章是港中文的贾佳亚团队、香港大学、旷世科技张翔雨团队合作的3D检测算法，主要创新点是：将经典的SECOND结构改为极简架构，以全稀疏的方式直接从voxel特征检测目标。实验指标也不错，在NuScene上的检测效果优于CenterPoint，劣于PillarNet，但是速度很快。</p>

<div class="component-content pagebody component">
  <h1 id="摘要" class="pagebody-header">
    摘要
  </h1>
</div><p class="component-content component"><strong>背景</strong>：3D检测算法总是依赖于anchor或center，并且大都是将2D框架转换为3D框架。因此需要将稀疏的voxel特征转换为稠密特征，然后用基于稠密特征的head输出检测结果。这些稠密数据导致计算量增加。</p>
<p class="component-content component"><strong>本文方法</strong>：本文提出完全基于稀疏数据的3D检测框架VoxelNext，该框架完全基于稀疏voxel特征进行检测，不必将稀疏特征进行其他转换或anchor、nms等操作。</p>
<p class="component-content component"><strong>实验结果</strong>：实现检测精度和检测速度的较好平衡，在NuScene上的跟踪指标最高，在WOD和AV2上也表现不错。 代码仓：https://github.com/dvlab-research/VoxelNeXt</p>

<div class="component-content pagebody component">
  <h1 id="基本介绍" class="pagebody-header">
    基本介绍
  </h1>
</div><p class="component-content component"><strong>传统方法的问题</strong>：最近主流的3D检测框架基本都基于3D稀疏卷积进行特征提取，然后借鉴Faster RCNN或CenterNet的思路设计head。这些方法有太多的人工设计和数据转换，数据处理不够直接，架构设计不够优雅，计算效率不够高。</p>
<p class="component-content component">anchor和center机制原本是为图像这样的规则数据设计的，没考虑3D数据的非规律性和稀疏性。为使用anchor和center机制，主流方法需要将3D稀疏特征转换为2D稠密特征，然后接上述的检测头。</p>
<p class="component-content component">这些方法尽管取得了不错的检测效果，但是也引入了问题，比如计算效率低、结构复杂等。</p>
<p class="component-content component">下面举例具体说明： 








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-68e43c96dc42629bf67acfddf8d3eba4" id="lht68e43c96dc42629bf67acfddf8d3eba4">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094804366.png" alt="image-20230522094804366"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094804366
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">上图中的CenterPoint的热力图可显示大部分空间的预测分数为0，占比约1%。然后转换为稠密特征后，需要在所有位置进行运算，这一方面增加了计算量，另一方面导致重复检测，又引入nms之类的后处理机制删除多余检测目标。</p>
<p class="component-content component"><strong>本文方法</strong>：为进一步优化3D检测框架，该团队提出VoxelNext结构，该方法简单高效、不需要后处理。本算法的核心是voxel-object机制，统统稀疏卷积网络直接从voxel特征检测目标。</p>
<p class="component-content component">本框架的优点是简单，没有稀疏特征到稠密特征的转换，不需要anchor、proposal和nms等复杂的组件。整体结构如下图所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-a8f37d0125668867871f67b85ea829ac" id="lhta8f37d0125668867871f67b85ea829ac">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094826144.png" alt="image-20230522094826144"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094826144
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>其他全稀疏方法</strong>：最近FSD也采用了全稀疏的框架，然后基于VoteNet的投票机制对proposal进行refine。又因为点云的分布可能不能完全表征目标形状，所以上述的投票机制需要不断迭代来提高检测精度。FSD的计算效率不如该团队的VoxelNext,  对比数据如下图所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-a71d0eb5d30806664cc1e65730ea02ef" id="lhta71d0eb5d30806664cc1e65730ea02ef">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094846644.png" alt="image-20230522094846644"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094846644
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="相关工作" class="pagebody-header">
    相关工作
  </h1>
</div><p class="component-content component"><strong>voxel转2D框架</strong>：3D检测框架大多与2D检测框架类似，比如RCNN系列的PV-RCNN、Voxel RCNN、Pyramid RCNN、Graph RCNN等；还有CenterPoint系列的CenterNet、Center Point等。</p>
<p class="component-content component">3D检测与2D检测问题的不同点在于点云的稀疏性，但是很多方法的检测头还是基于2D稠密卷积。</p>
<p class="component-content component">比如VoxelNet借鉴PointNet的方法提取voxel特征，然后转到2D结构；SECOND将VoxelNet中的voxel特征提取方法改为3D稀疏卷积。后来的SOTA方法基本都是沿用此结构。</p>
<p class="component-content component"><strong>voxel转热力图框架</strong>：CenterPoint将voxel特征转换为稠密特征，然后预测热力图预测目标质心。</p>
<p class="component-content component">此类方法被一些BEV融合方法借鉴，比如BEVFusion系列。</p>
<p class="component-content component"><strong>稀疏检测框架</strong>：之前提出的稀疏检测框架虽然方法各不相同，但是都很复杂。典型方法如下：</p>
<div class="component-content component"><ul>
<li>RSN在range图像上使用前景分割，然后基于分割的稀疏数据进行3D检测；</li>
<li>SWFormer结合稀疏transformer、分割窗口、multi-head和feature pyramid机制；</li>
<li>FSD使用点云聚类解决目标中心特征丢失问题；</li>
</ul></div>
<p class="component-content component"><strong>３D卷积网络</strong>：３D稀疏卷积提取３D特征效率很高，但是对于检测来说，提供的信息不够。</p>
<p class="component-content component">所以常用的３D检测框架都将３D稀疏特征转换成２D稠密特征，来对特征进行加强。</p>
<p class="component-content component">还有一些方法对稀疏卷积网络进行优化的，比如：</p>
<div class="component-content component"><ul>
<li>Focal　Sparse　ConvolutionａｌNetworks　for　３D　Object　Detection</li>
<li>Spatial　Pruned　Sparse　Convolution　for　Efficient　３D　Object　Detection</li>
</ul></div>
<p class="component-content component">还有一些方法用transformer替代３D稀疏卷积，比如</p>
<div class="component-content component"><ul>
<li>Voxle　Set　Transformer：A　Set　ｔｏ　Set　Approach</li>
<li>VoTr</li>
</ul></div>
<p class="component-content component">针对上述的感受野不足以及各种解决办法，该团队论证了<strong>只需要增加下采样层</strong>就可以解决，无需其他复杂设计。</p>
<p class="component-content component"><strong>３D目标跟踪方法</strong>：大多数MOT方法都直接基于检测结果进行卡尔曼滤波，CenterPoint预测目标速度后进行关联，然后复用CenterTrack结构。</p>

<div class="component-content pagebody component">
  <h1 id="voxelnet设计细节" class="pagebody-header">
    VoxelNet设计细节
  </h1>
</div><p class="component-content component">点云或者voxel在目标表面的分布基本都是不规律的、离散的。因此该团队想基于voxel提出一个不用anchor的检测框架。</p>
<p class="component-content component">该团队希望基于当前的３D稀疏卷积进行最小修改，实现基于voxel直接进行目标检测。本框架主要包含３个创新点：BackBone，稀疏检测头和跟踪模块。下面分别进行详细介绍：</p>

<div class="component-content pagebody component">
  <h2 id="稀疏卷积backbone的适配" class="pagebody-header">
    稀疏卷积BackBone的适配
  </h2>
</div><p class="component-content component">基于稀疏的voxel特征直接进行精确的3D检测要求特征必须表征能力强且感受野够大。</p>
<p class="component-content component">虽然3D稀疏卷积特征用于3D目标检测已经持续很多年了，但是最近的研究表明其特征信息不够充分，需要用Focal Sparse Convolution、Transformer等机制进行进一步增强。</p>
<p class="component-content component">3D BackBone整体结构如下图所示：</p>
<p class="component-content component">![【VoxelNext 3D BackBone】.excali](<a href="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/">https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/</a>【VoxelNext 3D BackBone】.excali.svg)</p>
<p class="component-content component">其中，红色背景模块为3D BackBone的修改部分，包括下采样层的Conv4/Conv5模块，高度压缩对应的Bev_out模块，voxel剪枝对应的DynamicFocalPruningDownsample模块。下面将具体介绍这三个改动点。</p>

<div class="component-content pagebody component">
  <h3 id="增加下采样层" class="pagebody-header">
    <strong>增加下采样层</strong>
  </h3>
</div><p class="component-content component"><strong>设计理论</strong>：针对此问题，该团队的解决办法是增加一个下采样层。 一般来说，3D稀疏卷积包含4层，对应的特征步长分别为1、2、4、8；这种结构产生的特征不足以进行航向预测。为对特征进行增强，该团队对原结构增加两个下采样层，对应的步长是16和32。</p>
<p class="component-content component">这个小改动极大地增加了感受野，效果示意图如下所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-2ed548b8a904cbaa34cc8068d3cabe18" id="lht2ed548b8a904cbaa34cc8068d3cabe18">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094911441.png" alt="image-20230522094911441"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094911441
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>具体实现</strong>：结合BackBone结构图，我们可知论文并没有将下采样层的所有设计细节呈现出来。从实现方面看，BackBone下采样层总共有3个修改点：</p>
<div class="component-content component"><ul>
<li>
<p class="component-content component">下采样层：论文里所说的增加两层下采样层，在代码里的实现方式是增加Conv5和Conv6两个稀疏卷积模块。这两个卷积模块的输入输出通道数都是128，与Conv4一样，这样设计是为了下一步的特征级联。</p>
</li>
<li>
<p class="component-content component">特征相加：特征级联的目的是将Conv5和Conv6的特征加到Conv4的对应位置上。由于Conv5和Conv6分别进行了步长为2的卷积下采样，所以特征相加之前要将Conv5和Conv6的位置进行恢复。特征相加的代码如下：</p>
<div class="component-content pagebody component code">
  
  
  
  <div class=""><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_conv5<span style="color:#f92672">.</span>indices[:, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">*=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>x_conv6<span style="color:#f92672">.</span>indices[:, <span style="color:#ae81ff">1</span>:] <span style="color:#f92672">*=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>x_conv4 <span style="color:#f92672">=</span> x_conv4<span style="color:#f92672">.</span>replace_feature(torch<span style="color:#f92672">.</span>cat([x_conv4<span style="color:#f92672">.</span>features, x_conv5<span style="color:#f92672">.</span>features, x_conv6<span style="color:#f92672">.</span>features]))
</span></span><span style="display:flex;"><span>x_conv4<span style="color:#f92672">.</span>indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x_conv4<span style="color:#f92672">.</span>indices, x_conv5<span style="color:#f92672">.</span>indices, x_conv6<span style="color:#f92672">.</span>indices])</span></span></code></pre></div></div>
  
</div></li>
<li>
<p class="component-content component">Shared_Conv: Shared_conv是卷积核为3，输入输出通道为128的submanifold稀疏卷积模块，论文里似乎没说这个所谓的共享卷积层是干什么的。我们且往下看。</p>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h3 id="高度压缩" class="pagebody-header">
    <strong>高度压缩</strong>
  </h3>
</div><p class="component-content component">典型3D检测框架将3D特征转换为2D稠密特征，但是该团队发现2D稀疏特征就足以进行3D检测。因此该团队将voxel特征投影到平面上，相同位置的特征相加。高度压缩代码如下：</p>
<div class="component-content pagebody component code">
  
  
  
  <div class=""><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bev_out</span>(self, x_conv):
</span></span><span style="display:flex;"><span>    features_cat <span style="color:#f92672">=</span> x_conv<span style="color:#f92672">.</span>features
</span></span><span style="display:flex;"><span>    indices_cat <span style="color:#f92672">=</span> x_conv<span style="color:#f92672">.</span>indices[:, [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>]]
</span></span><span style="display:flex;"><span>    spatial_shape <span style="color:#f92672">=</span> x_conv<span style="color:#f92672">.</span>spatial_shape[<span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    indices_unique, _inv <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>unique(indices_cat, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, return_inverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    features_unique <span style="color:#f92672">=</span> features_cat<span style="color:#f92672">.</span>new_zeros((indices_unique<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], features_cat<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>    features_unique<span style="color:#f92672">.</span>index_add_(<span style="color:#ae81ff">0</span>, _inv, features_cat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x_out <span style="color:#f92672">=</span> spconv<span style="color:#f92672">.</span>SparseConvTensor(
</span></span><span style="display:flex;"><span>        features<span style="color:#f92672">=</span>features_unique,
</span></span><span style="display:flex;"><span>        indices<span style="color:#f92672">=</span>indices_unique,
</span></span><span style="display:flex;"><span>        spatial_shape<span style="color:#f92672">=</span>spatial_shape,
</span></span><span style="display:flex;"><span>        batch_size<span style="color:#f92672">=</span>x_conv<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x_out</span></span></code></pre></div></div>
  
</div><p class="component-content component">【问】逻辑不通呀，3D特征不够，求和成2D特征就够？</p>

<div class="component-content pagebody component">
  <h3 id="voxel剪枝" class="pagebody-header">
    <strong>voxel剪枝</strong>
  </h3>
</div><p class="component-content component"><strong>设计理论</strong>：3D点云包含大量背景点，对目标检测没用。该团队用下采样层不断裁剪没用的voxel，按照VS-Net中SPS-Conv的机制，该团队有效控制了voxel数量。示意图如下所示








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-e5d59ec87eb6f00841df035f4f92d21c" id="lhte5d59ec87eb6f00841df035f4f92d21c">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094928753.png" alt="image-20230522094928753"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094928753
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>具体实现</strong>：voxel剪枝模块模块以降低检测精度为代价来提高推理速度。从实现来看，该模块比较复杂，而且效果有限，可能有更好的选择，比如VirConv的近端下采样方式。因此本文不对VoxelNext的voxel剪枝细节展开，只简单告诉读者有这个东西，类的定义在DynamicFocalPruningDownsample类中，基本结构如下图所示：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-ad821ebe9d095889a7869d4f0e62f1f7" id="lhtad821ebe9d095889a7869d4f0e62f1f7">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230529155917347.png" alt="image-20230529155917347"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230529155917347
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>实验数据</strong>：经过实验分析，大概可以减少一半的voxel，数据如下表：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-fc69d9f52ba8ea311bc96b26687f1c1f" id="lhtfc69d9f52ba8ea311bc96b26687f1c1f">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522094947274.png" alt="image-20230522094947274"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522094947274
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h2 id="稀疏检测头" class="pagebody-header">
    稀疏检测头
  </h2>
</div><p class="component-content component">该团队直接基于3D稀疏特征进行检测，训练时，将voxel assign到最近的真值框。损失函数使用Focal Loss。</p>
<p class="component-content component"><strong>voxel选择</strong>：该团队观察到，query voxel通常不在目标中心，甚至不在目标框内。关于NuScene数据集中query voxel 与目标框的位置关系统计信息如下表所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-21e70206ecd89e04720d53857139adfd" id="lht21e70206ecd89e04720d53857139adfd">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095007697.png" alt="image-20230522095007697"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095007697
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">此外，推理时该团队使用max-pooling来代替NMS。与Submanifold稀疏卷积类似，max-pooling只在非空地方执行，从而节省了head的计算量。</p>
<p class="component-content component"><strong>box回归</strong>：按照CenterPoint的原则，该团队对平面位置、高度、朝向角进行回归；此外对于NuScene数据集或跟踪任务，该团队还回归速度信息。这些预测结果的损失函数基于L1损失函数。</p>
<p class="component-content component">对于Waymo数据集，该团队对于IOU的预测通过kernel size为3的submanifold卷积层。该方式比全连接层精度高，时间消耗差不多，具体数据如下：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-0783af332d8dd1a58bdfed32bc373894" id="lht0783af332d8dd1a58bdfed32bc373894">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095026161.png" alt="image-20230522095026161"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095026161
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">VoxelNext的head结构如下图所示：</p>
<p class="component-content component">![【VoxelNext Head结构】.excalidraw](<a href="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/">https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/</a>【VoxelNext Head结构】.excalidraw.svg)</p>

<div class="component-content pagebody component">
  <h2 id="3d跟踪" class="pagebody-header">
    3D跟踪
  </h2>
</div><p class="component-content component">跟踪模块是基于CenterPoint的扩展，基于速度对目标位置进行预测，基于L2距离计s算关联关系。示意图如下所示：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-f3ebe22ef0180e70609663dc5d71607f" id="lhtf3ebe22ef0180e70609663dc5d71607f">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095045639.png" alt="image-20230522095045639"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095045639
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="实验结果" class="pagebody-header">
    实验结果
  </h1>
</div><p class="component-content component">下采样层的消融实验如下表所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-50a00dcf92d2b33812003ab29f89b4e2" id="lht50a00dcf92d2b33812003ab29f89b4e2">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095105421.png" alt="image-20230522095105421"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095105421
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">voxel剪枝的消融实验如下表：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-8f241a5d7ad8c2bd008addea4a86b5b7" id="lht8f241a5d7ad8c2bd008addea4a86b5b7">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095128793.png" alt="image-20230522095128793"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095128793
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-ae2e11a01903f68647224e71e4ef7ffb" id="lhtae2e11a01903f68647224e71e4ef7ffb">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095144582.png" alt="image-20230522095144582"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095144582
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">本文选择的剪枝比例是0.5。</p>
<p class="component-content component">高度压缩的消融实验：</p>
<p class="component-content component">如果BackBone和head都用3D稀疏卷积的话，计算量太大，因此将检测头设计成基于2D的。</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-2da96272e8b74df6b85c708502d88444" id="lht2da96272e8b74df6b85c708502d88444">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095204068.png" alt="image-20230522095204068"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095204068
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">在各数据集上的检测指标如下：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-d430aad350dd638dd07a55f8b6f886c2" id="lhtd430aad350dd638dd07a55f8b6f886c2">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095220122.png" alt="image-20230522095220122"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095220122
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-02a13cff9c48ef1bf911a40fcede4b54" id="lht02a13cff9c48ef1bf911a40fcede4b54">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095232232.png" alt="image-20230522095232232"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095232232
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-6a79ce90cb76b3c43c29b89772c5d637" id="lht6a79ce90cb76b3c43c29b89772c5d637">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230522095243299.png" alt="image-20230522095243299"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230522095243299
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="个人分析于总结" class="pagebody-header">
    个人分析于总结
  </h1>
</div><p class="component-content component"><strong>作者信息</strong>：这篇文章是港中文的贾佳亚团队和旷世科技的张翔雨团队合作的。两位都是领域大佬，贾佳亚大佬之前提出很多点云3D检测方法，比如IPOD、3DSSD、STD等，张翔雨大佬之前提出过LC融合3D检测方法PETR。值得注意的是，贾佳亚团队之前提出的3D检测算法都结构复杂、计算量大、难以商业落地，但是这次提出的VoxelNext主打一个简单快速好落地。这个模型设计上的思路转变还是挺大的，至于所提设计是否有效，且让该团队拭目以待。</p>
<p class="component-content component"><strong>创新点</strong>：提出基于voxel的稀疏检测框架。之前Voxel框架都基于SECOND结构，先3D稀疏卷积，然后转成2D稠密特征后进行2D卷积，再接proposal、head那一套流程。VoxelNext的全稀疏voxel框架砍掉了后面2D的那一连串流程，大大简化了结构、节省了时间。</p>
<p class="component-content component"><strong>大话演进方向</strong>：上面的内容都是基于论文内容的客观陈述，接下来是个人主观分析，请读者持辨证的态度阅读。分析基于个人掌握的知识，从本文出发，对3D检测技术的演进方向进行主观分析，因此称为“大话”。但是后续我会进行独立实验及相关的研究跟踪，预知后续分析结果，请保持关注。</p>
<p class="component-content component">话不多说，上菜：</p>
<div class="component-content component"><ul>
<li>与LC融合方法结合：当前的LC融合方法越来越成熟，检测指标也逐渐超越了基于单Lidar的检测算法。但是目前主流的LC融合方法的L分支还是基于SECOND结构，即voxel特征转2D的结构。此文的方法如果有效，则可以对目前所有基于SECOND结构的3D检测方法进行框架升级，从而提高检测精度，并且大大减少推理时间。这个方向是该文章最具吸引力的演进方向，该团队可以尝试做点实验进一步验证；</li>
<li>与其他全稀疏检测技术结合：该文章主打的还是基于voxel形式的全稀疏框架，但是最近也出现一些其他的全稀疏检测技术，而且效果不错。VoxelNext可以尝试与其他全稀疏技术进行碰撞，说不定会有更精彩的火花；</li>
<li>嫁接到其他优秀的3D检测框架上：有些优秀的3D检测框架是在基于SECOND结构的框架做加法，比如MPPNet引入连续帧序列的方法提高检测精度、GLENet对输入数据做文章。可以将那些优秀尝试移植到VoxelNext框架上，看看能开出什么花来；</li>
<li>与其他”next”辈的框架融合：22年、23年的点云3D检测算法的演进有这样的趋势：基于经典的SECOND、PointPillars、PointNet框架几乎没有新的演进，仿佛经典框架已经触到天花板，没什么优化空间了。于是22年、23年的点云3D检测算法四面突击：有的尝试对输入数据做文章，比如SPG、PDA、BtcDet、GLENet等；有的尝试引入序列检测的技术，比如MPPNet；有的引入全稀疏概念，比如SST、SFD等；还有的尝试对经典框架进行升级，提出各种”next”概念，比如PointNext、PillarNext，以及PillarNet等。VoxelNext可以与其他next思路结合，取长补短，从而推出更高效、更有效的“next”基线框架；</li>
</ul></div>

          </div>
          
          <div class="component">
            <div class="component-content">
              <div class="article-copyright">
                <p class="content">
                  Copyright: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed" target="_blank">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>
                </p>
                <p class="content">Author:  jk049 </p>
                <p class="content">Posted on:  May 22, 2023</p>
              </div>
            </div>
          </div>
          
        </article>
      </section>
  </main>

  <script>
    var script = document.createElement("script");script.src = "https://jk049.github.io/js/initPost.js";
    document.head.appendChild(script);
  </script>

    
    <div class="footer-main ">
  <div class="content-body footer-wraper">
    <div class="footer-box">
      <div class="foot-nav">
        <div class="foot-nav-items">
          <div class="item">
            <div class="logo"></div>
            <div class="email">Email: <a href="mailto:jk049jk@gmail.com">jk049jk@gmail.com</a></div>
          </div>

          <div class="item community">
            <div class="item-title">Social Media</div>
            
              <a href="https://github.com/floyd-li" target="_blank">Github</a>
            
              <a href="https://twitter.com/some-one" target="_blank">Twitter</a>
            
          </div>

          <div class="item resources">
            <div class="item-title">Related</div>
            
              <a href="https://yufengbiji.com/" target="_blank">驭风笔记</a>
            
              <a href="https://apple.com/" target="_blank">Apple</a>
            
          </div>
        </div>
      </div>
      <div class="bottom">
        <div class="item copyright">
          &copy; 2023
          Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> & <a href="https://github.com/floyd-li/hugo-theme-itheme" target="_blank">iTheme</a>
        </div>
      </div>
    </div>
  </div>
</div>

  </body>
    
    

    
    
</html>

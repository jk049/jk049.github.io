<!DOCTYPE html>
<html class="js no-touch  progressive-image  no-reduced-motion progressive" lang="en">
  <head>
    <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="icon" href="/img/favicon.ico">

    <meta name="keyword" content="">

    <title>MPPNet论文解读</title>

    <link rel="canonical" href="/posts/mppnet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">

    <link rel="stylesheet" href="/css/global.css">

    <link rel="stylesheet" href="/css/custom.css">

    <link rel="stylesheet" href="/css/search.css" />

    
    

    
    

</head>
  </head>
  <body class=" page-article   ">
    <header>
      <nav class="nav">
  <div class="nav-wrapper">
    <div class="nav-content-wrapper">
      <div class="nav-content">
        <a href="/ " class="nav-title">jk049&#39;s blog</a>
        <div class="nav-menu">
          <div class="nav-item-wrapper">
            <a href="/posts " class="nav-item-content">Articles</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/about" class="nav-item-content">About</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/index.xml" class="nav-item-content" target="_blank">RSS</a>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</nav>

<script>
  function toggleSearchModal(){
    const template = `
    <div class="modal-body">
      <div id="autocomplete" onclick="event.stopPropagation();"></div>
    </div>
    `
    const modal = document.querySelector("#modal-wrapper")
    if(!modal){
      const div = document.createElement("div")
      document.body.setAttribute("style","overflow: hidden;")
      div.setAttribute("id", "modal-wrapper")
      div.setAttribute("onclick", "toggleSearchModal()")
      div.innerHTML = template
      const script = document.createElement("script");script.setAttribute("src", "https://jk049.github.io/js/algolia.js")
      div.appendChild(script)
      document.body.append(div)
    } else {
      document.body.removeAttribute("style")
      document.body.removeChild(modal)
    }
  }
</script>
    </header>
    
  
  
  <main id="main" class="main">
      <section>
        <article class="article">
          
          <div class=" article-header ">
            <div class="category component">
              <div class="component-content">
                <div class="category-eyebrow">
                  <span class="category-eyebrow__category category_original">
                    
                      
                        Sequence Detection
                      
                    
                  </span>
                  <span class="category-eyebrow__date">May 5, 2023</span>
                </div>
              </div>
            </div>
            <div class="pagetitle component">
              <div class="component-content">
                <h1 class="hero-headline">MPPNet论文解读</h1>
              </div>
            </div>
            <div class="component  article-subhead ">
              <div class="component-content">MPPNet详细解读——从论文到代码</div>
            </div>

            <div class="tagssheet component">
              <div class="component-content">
                
                  
                  <a href="/tags/sequence-detection" class="tag">
                    Sequence Detection
                  </a>
                
                  
                  <a href="/tags/lidar" class="tag">
                    Lidar
                  </a>
                
                  
                  <a href="/tags/3d" class="tag">
                    3D
                  </a>
                
              </div>
            </div>
          </div>
          
          <div class="pagebody">
            
            
            
            
            
            
            
            
            
<div class="component-content pagebody component">
  <h1 id="摘要" class="pagebody-header">
    摘要
  </h1>
</div><p class="component-content component">3D检测的可靠性和准确性对很多应用都很重要，比如自动驾驶、服务机器人等。</p>
<p class="component-content component"><strong>MPPNet结构</strong>:本文提出MPPNet，一种高效且灵活的3D检测框架。 该框架是一个三级架构，通过代理点云(proxy points)实现多帧点云的特征交互和编码。三个模块的主要功能分别是：</p>
<div class="component-content component"><ul>
<li>当前帧的特征编码；</li>
<li>短序列的特征融合；</li>
<li>全序列的特征融合；</li>
</ul></div>
<p class="component-content component"><strong>关键模块</strong>：为保障全序列点云处理的资源消耗合理可控，在上述的第二级和第三级序列点云特征融合模块中，我们提出两个核心模块：</p>
<div class="component-content component"><ul>
<li>组内特征混合(intra-group feature mixing)</li>
<li>组间特征注意(inter-group feature attention)</li>
</ul></div>
<p class="component-content component"><strong>核心思路</strong>：引入多帧间交互的代理点云(proxy points)，其作用是：保证目标在连续帧上表征的一致性及多帧特征交互的连接桥梁。</p>
<p class="component-content component"><strong>实验结果</strong>：WOD上SOTA</p>
<p class="component-content component"><strong>代码</strong>：https://github.com/open-mmlab/OpenPCDet</p>

<div class="component-content pagebody component">
  <h1 id="基本介绍" class="pagebody-header">
    基本介绍
  </h1>
</div><p class="component-content component"><strong>基于点云序列的3D检测方法</strong>：最近有几篇文章证明：运用多帧连续点云可以有效提高3D检测的效果，这些文章包括：</p>
<div class="component-content component"><ul>
<li>AFDetV2;</li>
<li>CenterPoint?CenterFormer?</li>
<li>Rsn: Range sparse net for efficient, accurate lidar 3d object detection</li>
</ul></div>
<p class="component-content component"><strong>序列检测面临的问题</strong>——<strong>长尾效应</strong>：对于运动目标，多帧的点云序列会导致运动长尾现象，这种现象会影响检测精度。正因为此问题，上面提到的方法大多把序列长度控制在4帧以内，超过4帧会由于长尾效应导致检测效果下降。长尾效应的现象如下图所示：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-fb58c68bc4d1a484ffcf74f40e25bc3a" id="lhtfb58c68bc4d1a484ffcf74f40e25bc3a">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504141935024.png" alt="image-20230504141935024"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504141935024
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>本文的解决方法</strong>：本文的MPPNet处理流程分两阶段：</p>
<div class="component-content component"><ol>
<li>
<p class="component-content component"><strong>3D轨迹proposal</strong>：第一阶段复用之前的一阶段3D检测算法，获得3D轨迹的proposal。具体是使用PoiontPillars和CenterPoint的检测器生成proposal box，然后以proposal box及速度进行关联计，生成proposal轨迹；</p>
<p class="component-content component">该阶段的难点在于：多帧间的点云特征聚合不好做，因为不同帧之间点云的分布情况也不一样。</p>
<p class="component-content component">为解决分布对齐及聚合问题，我们提出proxy points概念：在proposal box的固定位置和相对一直的位置生成proxy point，使多帧间的特征聚合更容易一些。</p>
</li>
<li>
<p class="component-content component"><strong>多帧特征聚合</strong>：第二阶段是本文核心，输入3D轨迹的proposal之后，按目标聚合多帧特征，进而产生更准确的检测结果。第二阶段分为3个模块：</p>
<div class="component-content component"><ul>
<li>当前帧特征编码：采用PointNet++中的SA模块，使用proxy point进行编码，获得目标的几何特征；同时对帧间相对位置进行编码，获得目标的运动特征；</li>
<li>短序列特征聚合：由于直接使用proxy point进行特征汇聚会带来很大的计算负担，因此将proposal轨迹分割成小的轨迹片段，然后用3D MLP模块进行特征聚合；</li>
<li>全序列特征聚合：使用交叉注意力模块对上一步生成的轨迹片段特征进行进一步聚合；</li>
</ul></div>
</li>
</ol></div>
<p class="component-content component">通过上述3个阶段的特征聚合，我们获得了丰富的轨迹特征，这为之后产生高质量的检测框提供了很好的基础。</p>

<div class="component-content pagebody component">
  <h1 id="相关工作" class="pagebody-header">
    相关工作
  </h1>
</div>
<div class="component-content pagebody component">
  <h2 id="基于单帧点云的3d检测方法" class="pagebody-header">
    基于单帧点云的3D检测方法
  </h2>
</div><p class="component-content component">基于单帧的3D检测方法主要分3类：</p>
<div class="component-content component"><ul>
<li><strong>基于point的方法</strong>：代表性的方法有PointRCNN, STD, Vote3Deep， PointNet等，由于有PointNet++提出的SA等优秀算子，使得此类方法可以获得高质量的空间信息；</li>
<li><strong>基于voxel/pillar的方法</strong>：PIXOR，PointPillars, AfDet将3D转换为2D形式再检测；SECOND，CenterPoint, VoxelNet使用3D CNN提取3D特征；</li>
<li><strong>结合voxel和point的方法</strong>：PointNet++, PV-RCNN++, Lidar RCNN使用基于voxel的方法提取3D特征，使用基于point的方法进行box refine;</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="基于视频的2d检测方法" class="pagebody-header">
    基于视频的2D检测方法
  </h2>
</div><p class="component-content component">有一系列基于视频的2D检测方法，总的来说可分为两类：</p>
<div class="component-content component"><ul>
<li><strong>硬关联方法</strong>：这些方法的核心思路是使用之前帧的外观和运动特征来对齐当前帧的目标，运用的主要方法是光流、运动及LSTM来进行多帧的对齐和信息聚合。代表性方法有：
<div class="component-content component"><ul>
<li>Object detection in videos with tubelet proposal networks</li>
<li>T-cnn: Tubelets with convolutional neural networks for object detection from videos</li>
<li>Mining inter-video proposal relations for video object detection</li>
<li>New generation deep learning for video object detection: A survey</li>
<li>Fully motion-aware network for video object detection</li>
<li>Flow-guided feature aggregation for video object detection</li>
<li>video object detection with an aligned spatial-temporal memory</li>
<li>Detect to track and track to detect</li>
<li>Object detection in video with spatiotemporal sampling networks</li>
<li>Impression network for video object detection</li>
<li>Flow-guided feature aggregation for video object detection</li>
<li>Deep feature flow for video recognition</li>
</ul></div>
</li>
<li><strong>基于self-attention的软关联方法</strong>：最近几年，很多方法开始采用self-attention来进行特征对齐，代表性方法有：
<div class="component-content component"><ul>
<li>Sequence level semantics aggregation for video object detection</li>
<li>Memory enhanced global-local aggregation for video object detection</li>
<li>Relation distillation networks for video object detection</li>
</ul></div>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="基于点云序列的3d检测" class="pagebody-header">
    基于点云序列的3D检测
  </h2>
</div><div class="component-content component"><ul>
<li><strong>短序列检测方法</strong>：一些SOTA文章已经证明：使用短序列能显著提高3D检测效果，比如：
<div class="component-content component"><ul>
<li>AfDetV2;</li>
<li>Rsn: Range sparse net for efficient, accurate lidar 3d object detection</li>
<li>CenterPoint;</li>
</ul></div>
</li>
<li><strong>长序列检测方法</strong>：当序列长度更长时，上述的短序列检测方法开始出现负增益。于是出现一些如下解决方法：
<div class="component-content component"><ul>
<li>3D-MAN：3d-man: 3d multi-frame attention network for object detection, 采用注意力机制和memory bank来对齐多视角的时空信息；</li>
<li>SimTrack: Exploring simple 3d multi-object tracking for autonomous driving, 提出一种基于点云的3D检测跟踪一体化框架；</li>
<li>Offboard3D: Offboard 3d object detection from point cloud sequences, 提出一种离线检测方法，以过去帧和未来帧作为输入，极大地提高了检测精度；</li>
</ul></div>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h1 id="整体结构" class="pagebody-header">
    整体结构
  </h1>
</div><p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-a6657f18c1eae008dc824d9421c31e38" id="lhta6657f18c1eae008dc824d9421c31e38">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504142014013.png" alt="image-20230504142014013"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504142014013
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h2 id="单帧特征编码" class="pagebody-header">
    单帧特征编码
  </h2>
</div><p class="component-content component">特征编码模块主要功能是基于proposal trajectory对目标进行特征编码，主要的编码特征是几何特征和运动特征。具体的编码方式如下：</p>
<div class="component-content component"><ul>
<li>
<p class="component-content component"><strong>几何特征编码</strong>：以proxy points进行几何特征编码, 其输入数据包括：</p>
<div class="component-content component"><ul>
<li>
<p class="component-content component">proposal box：每个proposal box有8个顶点，1个中心点，proposal box表示为</p>
<p class="component-content component">$B^t ={{b_j^t}}_{j=1}^9$；</p>
</li>
<li>
<p class="component-content component">proposal trajectory：T帧的轨迹有m个，数学形式表示为</p>
<p class="component-content component">$K^t ={{l_1^t, &hellip; , l_m^t}}_{t=1,&hellip;,T}$；</p>
</li>
</ul></div>
<p class="component-content component">处理流程如下：</p>
<div class="component-content component"><ol>
<li>
<p class="component-content component">计算框的变化信息：当前帧的各个porposal box与其历史轨迹框的各点的距离，${\Delta}l_i^t = Concat({l_i^t - b_j^t}_{j=1}^9)$；</p>
</li>
<li>
<p class="component-content component">轨迹框信息加上变化信息：将第一个的框的变化信息加入到框的信息上，得到增强的特征</p>
<p class="component-content component">$F^t = {{(l_i^t, \Delta_i^t)}}_{i=1}^m$;</p>
</li>
<li>
<p class="component-content component">使用PointNet++中提出的Set Abstraction对每个proxy point聚合其领域点，计算过程为</p>
<p class="component-content component">${g_k}^t = SetAbstraction(p_k^t, F^t), for k = 1, &hellip; , N$;</p>
</li>
</ol></div>
</li>
<li>
<p class="component-content component"><strong>运动特征编码</strong>：</p>
<div class="component-content component"><ol>
<li>
<p class="component-content component">计算proxy point与proxy box顶点、中心点的距离：${\Delta}p_k^t = Concat({p_k^t - b_j^t}_{j=1}^9)$；</p>
</li>
<li>
<p class="component-content component">加入时间因素：</p>
<p class="component-content component">${f_k}^t = MLP(Concat({\Delta}p_k^t, e^t)), for k = 1, &hellip; , N$;</p>
</li>
</ol></div>
</li>
<li>
<p class="component-content component"><strong>单帧特征汇总</strong>：将运动特征和几何特征结合起来，最终输出的proxy point特征是$r_k^t = g_k^t + f_k^t , for k = 1, &hellip; , N$; 单帧中所有目标的特征是$R^t = {{r_1}^t, &hellip; , r_N^t}$;</p>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="组内特征聚合" class="pagebody-header">
    组内特征聚合
  </h2>
</div><p class="component-content component">该模块负责分组后，对组内的proposal轨迹进行时间特征编码。 第i组的特征是$G^i = {r_i^t, &hellip; , r_N^t}$ ,  $ G^i \in R^{T^i, N, D}$ , 其中D是每个proxy point的特征维度。</p>
<p class="component-content component">编码的方式就是MLP，公式为$\hat{G^i} = MLP^{4d}(MLP(G^i))  ,  for  s = 1, &hellip; , S$ ;</p>

<div class="component-content pagebody component">
  <h2 id="组间特征聚合" class="pagebody-header">
    组间特征聚合
  </h2>
</div><p class="component-content component">该模块的功能是对所有组的proxy point进行特征聚合，来获取更丰富的信息。 该聚合功能通过cross-attention机制来实现，具体处理流程如下：</p>
<div class="component-content component"><ol>
<li>总结所有轨迹信息：通过MLP层对S组、N个轨迹、D维proxy point特征进行信息汇总和总结，所有轨迹信息为H, 则公式为$\hat{H} = MLP(H), \hat{H} \in R^{N * D}$ ;</li>
<li>为每个组的轨迹聚合全局信息：使用cross-attention机制将全局信息聚合到每个组里，公式为：$\hat{G^i} = MultiHeadAttn(Q(\hat{G^i} + PE), K(H + PE), V(H)), for  i = 1, &hellip; , S$; 其中PE是将proxy point索引经过MLP映射得到的proxy point位置编码信息；</li>
</ol></div>

<div class="component-content pagebody component">
  <h2 id="检测头" class="pagebody-header">
    检测头
  </h2>
</div><p class="component-content component">使用一个简单的transformer层来获得每组的特征向量，基本处理流程如下：</p>
<div class="component-content component"><ol>
<li>
<p class="component-content component">使用multi head attention从每组聚合信息，公式为</p>
<p class="component-content component">$ E^i = MultiHeadAttn(Q(E), K(\hat{G^i} + PE), V(\hat{G^i})), for  i = 1, &hellip; , S$</p>
</li>
<li></li>
</ol></div>

<div class="component-content pagebody component">
  <h2 id="损失函数" class="pagebody-header">
    损失函数
  </h2>
</div><p class="component-content component">损失值是置信度的损失值与BOX回归框损失的和，公式为$L = L_{conf} + \alpha*L_{reg}$; 其中置信度和box损失值的计算方式与CT3D中的一样。</p>

<div class="component-content pagebody component">
  <h1 id="实验结果" class="pagebody-header">
    实验结果
  </h1>
</div>
<div class="component-content pagebody component">
  <h2 id="数据集" class="pagebody-header">
    数据集
  </h2>
</div><p class="component-content component">WOD是一个规模很大的3D检测数据集，包含1150个序列，其中798个训练序列，202个验证序列，150个测试序列。 本模型在训练集上训练，在测试集和验证集上evaluate。</p>
<div class="component-content component"><ul>
<li><strong>官方评价指标</strong>：WOD官方的评价指标有两个：
<div class="component-content component"><ul>
<li>mAP：3D Average Precision的均值；</li>
<li>mAPH: mAP加权Heading准确度的指标；</li>
</ul></div>
</li>
<li><strong>难度区分</strong>：根据每个目标包含的point数量，数据集分成2个部分：
<div class="component-content component"><ul>
<li>LEVLE 1：每个目标包含point数量大于5；</li>
<li>LEVEL 2：每个目标至少包含1个point；</li>
</ul></div>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="实现细节" class="pagebody-header">
    实现细节
  </h2>
</div><div class="component-content component"><ul>
<li><strong>训练细节</strong>：两阶段的训练是分开的：
<div class="component-content component"><ul>
<li>第一阶段的RPN子网络按照CenterPoint和PointPillars的训练策略进行训练；</li>
<li>第二阶段子网络是以ADAM优化器进行6个epoch的训练，学习率为0.003， batchsize为16，针对proposal trajectory的IOU阈值为0.5；</li>
</ul></div>
</li>
<li><strong>超参数</strong>：
<div class="component-content component"><ul>
<li>每个proposal的proxy point数量N=64；</li>
<li>每个proposal box随机采样点的数量为128；</li>
<li>每组的帧窗口长度为4；</li>
</ul></div>
</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="测试结果" class="pagebody-header">
    测试结果
  </h2>
</div><div class="component-content component"><ul>
<li><strong>与PV-RCNN++对比</strong>：我们的第一阶段自网络使用CenterPoint, 输入序列长度设为16， 测试结果显示：LEVEL2样本的车、行人、自行车的mAPH分别提高5.05%，8.76%，4.9%。</li>
<li><strong>与3D-MAN对比</strong>：LEVEL2的vehicle的mAPH提高7.82%；</li>
<li><strong>与CT3D对比</strong>：我们将CT3D扩展成多帧的CT3D-MF，与扩展后的模型相比，vehicle的mAPH提高4.12%；</li>
</ul></div>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-54a42c868354f0f916610a9fa05dc1b9" id="lht54a42c868354f0f916610a9fa05dc1b9">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504142042900.png" alt="image-20230504142042900"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504142042900
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-a4c906e6f758f5100ef9fed9d1b187d2" id="lhta4c906e6f758f5100ef9fed9d1b187d2">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504142059343.png" alt="image-20230504142059343"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504142059343
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="个人分析与总结" class="pagebody-header">
    个人分析与总结
  </h1>
</div><p class="component-content component"><strong>作者信息</strong>：MPPNet是港中文的李鸿升团队、MPI的史少帅及华为诺亚实验室的徐航团队合作的文章，发表于2022年5月。这几位作者是基于lidar的3D检测领域的重量级人物，近几年提出了很多非常不错的3D检测方法。比如2018年的PointRCNN、2020年的PV-RCNN、Voxel RCNN、PV-RCNN++、2021年的VoTr、Pyramid RCNN、2022年的Point2Seq、2023年的DSVT等方法。</p>
<p class="component-content component">现在这篇基于点云序列的3D检测方法，是作者对SECOND结构的网络进行组件优化、组件transformer化之后的又一个尝试方向。</p>
<p class="component-content component"><strong>创新点</strong>:  对基于点云序列的3D检测方法进行了完善，提出了新的组件和框架，包括几何特征编码模块、运动特征编码模块、组内特征编码模块和组间特征编码模块。</p>
<p class="component-content component">具体的创新点为下图中红色框所代表的模块：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-7df492ba5f736a49cb87f11087e3e58b" id="lht7df492ba5f736a49cb87f11087e3e58b">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504143232657.png" alt="image-20230504143232657"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504143232657
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>大话优化方向</strong>：上面的内容都是基于论文内容的客观陈述，接下来是个人主观分析，请读者持批判态度阅读。</p>
<p class="component-content component">分析基于个人掌握的知识，从本文出发，对3D检测技术的演进方向进行主观分析，因此称为“大话”。</p>
<p class="component-content component">但是后续我会进行独立实验及相关的研究跟踪，预知后续发展请保持关注。</p>
<p class="component-content component">话不多说，上菜：</p>
<p class="component-content component">当前基于点云序列的3D检测方法还比较少，没有公认的成熟的方法。本文使用基于MLP和cross attention的组件进行proposal序列的特征提取和聚合，并获得了明显的提升。后续可能的演进方向有：</p>
<div class="component-content component"><ul>
<li>基于MPPNet框架，提出更高效的序列特征提取和聚合机制：当前学术界对Transformer的理解还不够深刻，待Transformer发展更成熟后，基于Transformer的序列特征聚合机制也将进一步发展，也许到时候学术界会提出更好的针对点云序列的特征提取和聚合机制；</li>
<li>与点云补全方法结合：2022年一来，涌现出几篇基于点云补齐的3D检测方法，比如BtcDet/PDA/GLENet等，可尝试将此类方法与点云序列方法结合；</li>
<li>将序列检测机制与LC融合机制相结合：当前LC融合框架的检测指标逐渐超过只基于Lidar的3D检测指标，说明学者们逐渐提出了有效的LC融合机制。而基于点云序列的3D检测方法也被越来越多的人注意到，逐渐提出一些有效的检测框架。不难想像，后面这两种方向的3D检测方法还会继续演进。我们一方面可以不断迭代两种方向的检测方法，另一方面也可以将两种方法结合，也许会取得不错的效果，比如MPPNet+LoGoNet、MPPNet+VirConv等；</li>
</ul></div>

          </div>
          
          <div class="component">
            <div class="component-content">
              <div class="article-copyright">
                <p class="content">
                  Copyright: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed" target="_blank">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>
                </p>
                <p class="content">Author:  jk049 </p>
                <p class="content">Posted on:  May 5, 2023</p>
              </div>
            </div>
          </div>
          
        </article>
      </section>
  </main>

  <script>
    var script = document.createElement("script");script.src = "https://jk049.github.io/js/initPost.js";
    document.head.appendChild(script);
  </script>

    
    <div class="footer-main ">
  <div class="content-body footer-wraper">
    <div class="footer-box">
      <div class="foot-nav">
        <div class="foot-nav-items">
          <div class="item">
            <div class="logo"></div>
            <div class="email">Email: <a href="mailto:jk049jk@gmail.com">jk049jk@gmail.com</a></div>
          </div>

          <div class="item community">
            <div class="item-title">Social Media</div>
            
              <a href="https://github.com/floyd-li" target="_blank">Github</a>
            
              <a href="https://twitter.com/some-one" target="_blank">Twitter</a>
            
          </div>

          <div class="item resources">
            <div class="item-title">Related</div>
            
              <a href="https://yufengbiji.com/" target="_blank">驭风笔记</a>
            
              <a href="https://apple.com/" target="_blank">Apple</a>
            
          </div>
        </div>
      </div>
      <div class="bottom">
        <div class="item copyright">
          &copy; 2023
          Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> & <a href="https://github.com/floyd-li/hugo-theme-itheme" target="_blank">iTheme</a>
        </div>
      </div>
    </div>
  </div>
</div>

  </body>
    
    

    
    
</html>

<!DOCTYPE html>
<html class="js no-touch  progressive-image  no-reduced-motion progressive" lang="en">
  <head>
    <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="icon" href="/img/favicon.ico">

    <meta name="keyword" content="">

    <title>VirConv论文解读</title>

    <link rel="canonical" href="/posts/virconv%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">

    <link rel="stylesheet" href="/css/global.css">

    <link rel="stylesheet" href="/css/custom.css">

    <link rel="stylesheet" href="/css/search.css" />

    
    

    
    

</head>
  </head>
  <body class=" page-article   ">
    <header>
      <nav class="nav">
  <div class="nav-wrapper">
    <div class="nav-content-wrapper">
      <div class="nav-content">
        <a href="/ " class="nav-title">jk049&#39;s blog</a>
        <div class="nav-menu">
          <div class="nav-item-wrapper">
            <a href="/posts " class="nav-item-content">Articles</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/about" class="nav-item-content">About</a>
          </div>
          <div class="nav-item-wrapper">
            <a href="/index.xml" class="nav-item-content" target="_blank">RSS</a>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</nav>

<script>
  function toggleSearchModal(){
    const template = `
    <div class="modal-body">
      <div id="autocomplete" onclick="event.stopPropagation();"></div>
    </div>
    `
    const modal = document.querySelector("#modal-wrapper")
    if(!modal){
      const div = document.createElement("div")
      document.body.setAttribute("style","overflow: hidden;")
      div.setAttribute("id", "modal-wrapper")
      div.setAttribute("onclick", "toggleSearchModal()")
      div.innerHTML = template
      const script = document.createElement("script");script.setAttribute("src", "https://jk049.github.io/js/algolia.js")
      div.appendChild(script)
      document.body.append(div)
    } else {
      document.body.removeAttribute("style")
      document.body.removeChild(modal)
    }
  }
</script>
    </header>
    
  
  
  <main id="main" class="main">
      <section>
        <article class="article">
          
          <div class=" featured-header ">
            <div class="category component">
              <div class="component-content">
                <div class="category-eyebrow">
                  <span class="category-eyebrow__category category_original">
                    
                      
                        虚拟点云
                      
                    
                  </span>
                  <span class="category-eyebrow__date">March 27, 2023</span>
                </div>
              </div>
            </div>
            <div class="pagetitle component">
              <div class="component-content">
                <h1 class="hero-headline">VirConv论文解读</h1>
              </div>
            </div>
            <div class="component  featured-subhead ">
              <div class="component-content">一个基于图像生成虚拟点云的LC融合框架，提出新的点云去冗余和特征聚合方法</div>
            </div>

            <div class="tagssheet component">
              <div class="component-content">
                
                  
                  <a href="/tags/%E8%99%9A%E6%8B%9F%E7%82%B9%E4%BA%91" class="tag">
                    虚拟点云
                  </a>
                
                  
                  <a href="/tags/lidar" class="tag">
                    LiDAR
                  </a>
                
                  
                  <a href="/tags/3d" class="tag">
                    3D
                  </a>
                
                  
                  <a href="/tags/lc-fusion" class="tag">
                    LC Fusion
                  </a>
                
              </div>
            </div>
          </div>
          
          <div class="pagebody">
            
            
            
            
            
            
            
            
            
<div class="component-content pagebody component">
  <h1 id="摘要" class="pagebody-header">
    摘要
  </h1>
</div><p class="component-content component"><strong>问题背景</strong>：最近基于虚拟point进行LC融合的方法引起人们关注，但是这种方法有两个重大缺点：</p>
<div class="component-content component"><ul>
<li><strong>计算量大</strong>：基于图像产生的虚拟点云很稠密，导致计算量很大；</li>
<li><strong>噪声大</strong>：产生的虚拟点云深度信息不准确，导致检测结果不准确；</li>
</ul></div>
<p class="component-content component"><strong>本文设计</strong>：为解决上述问题，本文提出VirConvNet(Virtual Sparse Convolution Network)，这是一个快速且有效的backbone, 该框架主要有两个关键设计：</p>
<div class="component-content component"><ul>
<li>StVD(Stochastic Voxel Discard): 通过丢弃虚拟每个point附近冗余点来减少计算量；</li>
<li>NRConv(Noise Resistant Submanifold Convolution): 在图像空间和点云空间进行VFE来解决噪声问题；</li>
</ul></div>
<p class="component-content component"><strong>实现结果</strong>：我们实现了三种模式的代码：</p>
<div class="component-content component"><ul>
<li>VirConv-L: 基于已有的融合结构Voxel RCNN实现了VirConv-L;</li>
<li>VirConv-T: 更改了refine机制，基于Casa和TED实现了精度更高的VirConv-T;</li>
<li>VirConv-S: 基于pseudo-label框架3DIouMatch，实现了半监督框架VirConv-S;</li>
</ul></div>
<p class="component-content component">在KITTI上的AP排名第1第2，同时速度很快，在xxx上推理时间为56ms，介于Voxel RCNN和PV-RCNN之间。 代码仓开源：https://github.com/hailanyi/VirConv</p>

<div class="component-content pagebody component">
  <h1 id="基本介绍" class="pagebody-header">
    基本介绍
  </h1>
</div>
<div class="component-content pagebody component">
  <h2 id="领域背景" class="pagebody-header">
    领域背景
  </h2>
</div><p class="component-content component">lidar可在各种光照条件下提供可靠的定位信息，因此3D检测对自动驾驶领域很重要。</p>
<p class="component-content component"><strong>基于Lidar的3D检测无法克服点云稀疏的固有缺陷</strong>：虽然近几年基于Lidar的3D检测取得了很大进展，比如PointPillars/Pyramid RCNN/CT3D/PointRCNN/FromPointstoPart/3DSSD/STD等方法，但是由于点云的稀疏性，导致对远处目标的检测精度急剧下降。</p>
<p class="component-content component"><strong>LC特性互补</strong>：camera能提供高分辨率的采样数据及丰富的上下文信息，因此基于camera和lidar的特性互补的LC融合方法能取得很好的性能，比如TransFusion/Focal Sparse Convolution/MMF/BevFusion/Frustum PointNet等。</p>
<p class="component-content component"><strong>融合方法演进过程</strong>：</p>
<div class="component-content component"><ul>
<li><strong>以图像信息增强点云特征</strong>：早期的方法通常用图像特征(CNN特征或semantic mask)来增强点云特征，比如MvxNet/PointPainting/PointAugmenting等；</li>
<li><strong>虚拟点云生成</strong>：通过生成额外的点云来提高点云的密度，代表性方法有MVP/SFD等，这些方法显示出在3D检测领域的巨大潜力；但是这些方法生成的点云密度太高，导致计算量太大。因此一些方法通过下采样或设置更大的voxel尺寸来减少点云的生成数量，比如PointPillars/CenterPoint/RandLA-Net等，但是这种方法不可避免会丢失一些几何信息，导致检测精度下降或者定位精度下降。</li>
</ul></div>

<div class="component-content pagebody component">
  <h2 id="本文的解决思路" class="pagebody-header">
    本文的解决思路
  </h2>
</div><p class="component-content component"><strong>思路</strong>：论文团队观察到虚拟点云应用于3D检测的两个现象，基于这两个现象提出解决方法。这两个现象是：</p>
<div class="component-content component"><ul>
<li>
<p class="component-content component">目标周围的几何信息相对完整，因此大部分目标周围的虚拟点云只能带来有限的增益，但付出的计算消耗很大。作者这里贴了两张图，传达的意思是：3%的虚拟点云能带来2.2%的AP提升，剩下97%的虚拟点云只带来0.18%的AP提升，同时时间消耗大概增加2-3倍。图片如下：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-61a67c62271fa2d477d553f94fd38b51" id="lht61a67c62271fa2d477d553f94fd38b51">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204535359.png" alt="image-20230504204535359"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204535359
      </div>
    </div>
  </div>
</figure>
</p>
</li>
<li>
<p class="component-content component">虚拟点云中的噪点大都分布在目标的边缘，可以将点云投射到图像上来识别这些噪点。示意图如下所示：</p>
</li>
</ul></div>
<p class="component-content component"><strong>解决方法</strong>：针对上述两个现象，本问提出两个技术点：</p>
<div class="component-content component"><ul>
<li><strong>StVD冗余点丢弃机制</strong>：基于观察到的这两个现象，我们设计出StVD(Stochastic Voxel Discard)机制，该机制保留重要的点，即丢弃voxel附近的大量点，保留稍微远离voxel的虚拟点；</li>
<li><strong>NRConv(Noise Resistant Submanifold Convolution)几何特征编码层</strong>: 该层可以对3D空间和2D空间进行几何特征编码， 通过2D空间的扩展感受野区分噪声模式，进而抹掉噪声的影响。</li>
</ul></div>

<div class="component-content pagebody component">
  <h1 id="相关工作" class="pagebody-header">
    相关工作
  </h1>
</div><p class="component-content component"><strong>基于Lidar的3D检测方法</strong>：基于Lidar的3D检测方法主要分为如下几种：</p>
<div class="component-content component"><ul>
<li><strong>早期方法</strong>：早期的基于lidar的3D检测方法是将点云转换到BEV或range图像，然后再检测，比如BirdNet/MV3D;</li>
<li><strong>基于voxel的稀疏卷积方法</strong>：比如Voxel RCNN/Structure Aware Single-Stage 3D detection/PointPillars/SECOND等；</li>
<li><strong>基于点云SA的方法</strong>： 比如PV-RCNN/PointRCNN/3DSSD/STD等；这些方法虽然取得了不错的效果，但是无法解决点云稀疏性缺陷，对远处目标的检测精度很低。</li>
</ul></div>
<p class="component-content component"><strong>基于多模的3D检测方法</strong>：LC特性互补，可以用来提高3D检测的指标。该类方法主要分为如下几类：</p>
<div class="component-content component"><ul>
<li><strong>早期方法-特征增强</strong>：早期的方法通常是用图像特征来增强点云特征，比如MvxNet/PointPainting/PointAugmenting等；</li>
<li><strong>特征融合方法</strong>：LC各自独立进行特征编码，之后在局部ROI进行特征融合，比如FUTR3D/AVOD/BevFusion等；</li>
<li><strong>通过虚拟点云进行多模融合</strong>：最近提出的基于虚拟点云将L和C的数据融合到一起，比如Sparse Fuse Dense/Multimodal virtual point 3D detection，本文就是基于这种方法。虚拟点云通过点云补齐的方式增强远处目标的几何信息，已经显示出在3D检测领域的强大潜力。但是这种方法存在数据量大和噪点问题，本文的主要研究方向就是解决这两个问题。</li>
<li><strong>3D视觉领域的噪声处理方法</strong>：传统的去噪方式主要是各种滤波算法，比如：Bilateral Mesh Denoising/Guide 3d point cloud filtering/A review of algorithms for filtering the 3d point cloud等。最近有基于score和语义分割的点云去噪方法，也有基于目标边缘检测的去噪方法，但是这些方法会降低几何精度。</li>
</ul></div>
<p class="component-content component"><strong>半监督3D检测方法</strong>：最近有很多半监督方法通过大量未标注的数据来提升3D检测精度，因此我们参考3DIouMatch/Semi-supervised 3d Objection Detection via Adaptive Pseudo-labeling/Sess:Selfensembling Semi-supervised 3d Objection Detection的框架，也实现了一版基于半监督框架的VirConv-S。</p>

<div class="component-content pagebody component">
  <h1 id="virconv整体结构" class="pagebody-header">
    VirConv整体结构
  </h1>
</div><p class="component-content component">VirConv的框架依赖一个新的算子VirConv， 该算子是基于虚拟点云的3D目标检测算子，整体结构如下：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-3fd1e28d9f79adf18e5d171ea5bf1370" id="lht3fd1e28d9f79adf18e5d171ea5bf1370">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204626912.png" alt="image-20230504204626912"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204626912
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-639dad991b7f56ee1c92cf8a6ecb13c3" id="lht639dad991b7f56ee1c92cf8a6ecb13c3">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204640372.png" alt="image-20230504204640372"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204640372
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>虚拟点云</strong>：最近很多基于Lidar的3D检测框架都应用虚拟点云，基于虚拟点云的检测方法可以分为两类：</p>
<div class="component-content component"><ul>
<li>前融合：将原始点云与虚拟点云融合，生成融合点云，然后用现有的3D检测框架；</li>
<li>后融合：分别对原始点云和虚拟点云进行特征编码，然后在BEV平面或局部ROI进行特征融合，比如Sparse Fuse Dense；</li>
</ul></div>
<p class="component-content component">但是这两种方法都无法解决虚拟点云稠密问题和噪声问题。</p>

<div class="component-content pagebody component">
  <h2 id="stvd细节" class="pagebody-header">
    StVD细节
  </h2>
</div><p class="component-content component">该模块的作用是减少资源消耗，具体包含两个部分：</p>
<div class="component-content component"><ol>
<li>
<p class="component-content component"><strong>输入模块</strong>：丢弃部分虚拟点云组成的voxel，提高训练和推理速度。通常有两种实现方式：</p>
<div class="component-content component"><ul>
<li>随机采样：该方法对全局点云随机采样或随机丢弃，导致远处稀疏的点云完全丢失，示意图如下所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-8e426ef925bc5d720d15aef2036c515f" id="lht8e426ef925bc5d720d15aef2036c515f">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204702786.png" alt="image-20230504204702786"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204702786
      </div>
    </div>
  </div>
</figure>
</li>
<li>FPS（Farthest point sampling): FPS的计算量大，时间复杂度是$O(n^2)$;</li>
</ul></div>
<p class="component-content component">针对此问题，我们提出bin-based采样策略，具体机制是：</p>
<div class="component-content component"><ol>
<li>将voxel按照距离划分成$N^b$个部分，本文划分成10个部分；</li>
<li>近处采样固定数量的voxel，比如1000个；</li>
<li>远处的voxel全部保留；</li>
</ol></div>
<p class="component-content component">该方法能丢弃90%的虚拟点云，同时速度提高2倍，统计数据如下：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-5c7358637d549780a229858aeebfb724" id="lht5c7358637d549780a229858aeebfb724">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204734055.png" alt="image-20230504204734055"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204734055
      </div>
    </div>
  </div>
</figure>
</p>
</li>
<li>
<p class="component-content component"><strong>StVD模块</strong>：在VirConv模块丢弃部分虚拟voxel, 提高关于点云的鲁棒性。具体的实现方式是丢弃15%的voxel，相当于一种数据增强策略。丢弃比例与检测指标之间的关系如下图所示：








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-3598f45704cfec8e556782519eb27f5d" id="lht3598f45704cfec8e556782519eb27f5d">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204752787.png" alt="image-20230504204752787"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204752787
      </div>
    </div>
  </div>
</figure>
</p>
</li>
</ol></div>

<div class="component-content pagebody component">
  <h2 id="nrconv细节" class="pagebody-header">
    NRConv细节
  </h2>
</div><p class="component-content component">虚拟点云中的噪点很难从3d空间中识别出来，但是从2d图像中识别很简单。</p>
<p class="component-content component">我们基于submanifold稀疏卷积开发出noise-resistant convolution来解决虚拟噪点问题。</p>
<p class="component-content component">具体流程如下：</p>
<div class="component-content component"><ol>
<li><strong>几何特征编码</strong>：先用3D submanifold稀疏卷积提取每个voxel的几何特征；</li>
<li><strong>2d图像空间的噪声特征编码</strong>：将3d voxel转化为grid point, 然后基于LC标定参数将grid point投影到图像平面，用2D稀疏卷积进行特征提取；</li>
<li><strong>2D-3D特征级联</strong>：将两个维度的特征级联起来，生成抗噪特征；</li>
</ol></div>
<p class="component-content component">NRConv结构如下图所示：</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-e880f3146ea23f6b3f68410f2546156d" id="lhte880f3146ea23f6b3f68410f2546156d">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204811670.png" alt="image-20230504204811670"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204811670
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="实现及测试指标" class="pagebody-header">
    实现及测试指标
  </h1>
</div><p class="component-content component"><strong>数据集及测试指标</strong>：</p>
<div class="component-content component"><ul>
<li>数据集：KITTI 3D有7481帧训练集，7518帧测试集。我们将训练集划分为3712帧训练集和3769帧验证集，与voxel RCNN保持一致。此外，我们以KITTI odometry数据集作为无标签数据集，用于半监督学习。KITTI odometry包含43552帧数据，我们用10888帧进行半监督训练。</li>
<li>测试指标：我们以3d mAP(R40)作为评价指标，car/行人/自行车的IOU阈值分别为0.7/0.5/0.5；</li>
</ul></div>
<p class="component-content component"><strong>实现细节</strong>：</p>
<div class="component-content component"><ul>
<li>虚拟点云生成：用PENet生成虚拟点云，与SFD一样；</li>
<li>pipeline: BackBone与Voxel RCNN一样；</li>
<li>损失函数：Voxel RCNN和CasA的；</li>
<li>数据增强：旋转/平移/翻转；</li>
<li>训练超参数：Adam优化器，学习率0.01，epoch 60, proposal nms阈值0.8, 数量为160；</li>
<li>推理超参数：refine的NMS阈值为0.1；</li>
</ul></div>
<p class="component-content component"><strong>实验结果</strong>：比Voxel RCNN的mAP提高3-5个百分点。</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-08a8dd094fb49a69dd713fba5981c600" id="lht08a8dd094fb49a69dd713fba5981c600">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504210913944.png" alt="image-20230504210913944"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504210913944
      </div>
    </div>
  </div>
</figure>
</p>
<p class="component-content component"><strong>消融实验</strong>：StVD能减少一半时间，NRConv能提高2个mAP百分点，目标越远，提高的指标越多。</p>
<p class="component-content component">








<figure class="image component image-fullbleed body-copy-wide nr-scroll-animation nr-scroll-animation--on image-big">  <div class="component-content">
    <div class="image-sharesheet">
      <div class="image image-load image-asset image-4c303e76a14cd64a8cf31aa6388fcab3" id="lht4c303e76a14cd64a8cf31aa6388fcab3">
        <picture  class="picture">
          <img class="picture-image" data-src="https://blog-pic-bkt.oss-ap-southeast-1.aliyuncs.com/img/image-20230504204902459.png" alt="image-20230504204902459"  />
        </picture>
      </div>
    </div>
    <div class="image-description">
      <div class="image-caption">
        image-20230504204902459
      </div>
    </div>
  </div>
</figure>
</p>

<div class="component-content pagebody component">
  <h1 id="个人分析与总结" class="pagebody-header">
    个人分析与总结
  </h1>
</div><p class="component-content component"><strong>作者信息</strong>：该文章是厦门大学的王程团队与MPI的史少帅合作提出的。史少帅从2018年开始就提出很多基于Lidar的3D检测方法，比如PV-RCNN系列、Voxel RCNN等；值得注意的是，王程团队虽然之前没有知名的3D检测方法，但是近两年持续提出优秀的3D检测方法，比如该团队发表的另一篇3D检测文章，在KITTI 3D榜单上排名也很靠前。</p>
<p class="component-content component"><strong>创新点</strong>：该文章结构相对简单，创新点主要有两个：</p>
<div class="component-content component"><ul>
<li>StVD: 丢弃多余的虚拟点云;</li>
<li>NRConv: 一种3D特征和2D特征聚合方式，该聚合方式只是将3D特征和2D特征级联，3D特征与2D特征的对应关系通过标定参数获得，算是很简单的特征聚合方式。相比此聚合方式，同期的其他文章使用的聚合方法要复杂得多，比如LoGoNet使用cross-attention获得LC特征的对应关系并进行2D-3D特征聚合；MPPNet使用MLP和cross-attention组合的方式进行2D-3D特征聚合。</li>
</ul></div>
<p class="component-content component"><strong>进一步优化方向</strong>：该文章在KITTI 3D的榜单上排名很高，主要原因是基于图像产生虚拟点云，然后删除多余点云，用保留下的点云提取特征和目标检测。个人认为，进一步的优化方向有如下几个方面：</p>
<div class="component-content component"><ul>
<li>关于虚拟点云的生成方法的优化：该方法生成虚拟点云的方法复用SFD的方式，使用PENet生成虚拟点云。除了根据图像生成虚拟点云的方法外，还有基于Lidar点云的分布生成虚拟点云的方法，比如BtcDet/PDA/GLENet等。是否可以将这两种方法结合，同时根据图像和原始点云，生成质量更高的虚拟点云？这个方向可以进一步探索。</li>
<li>冗余点云删除方法的应用：本文使用StVD这一简单策略删除多余的点云，大大提高了训练和推理的速度。此方法是否也可以应用于基于原始点云的3D检测方法？将近处的多余点云删除来提高运算速度？</li>
<li>特征聚合方式的优化：本文使用映射和级联的方式进行2D-3D特征聚合。该聚合方法要做LC全局关联和对齐，全局对齐必然导致局部对齐精度低，理由参考LoGoNet。是否可以将LoGoNet的局部对齐的方式应用于VirConv？通过提高2D-3D特征的对齐精度和聚合效果来提高目标检测的精度？或者使用其他基于Transformer的方式进行更有效的特征聚合，从而提高检测精度？</li>
</ul></div>

          </div>
          
          <div class="component">
            <div class="component-content">
              <div class="article-copyright">
                <p class="content">
                  Copyright: <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed" target="_blank">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a>
                </p>
                <p class="content">Author:  jk049 </p>
                <p class="content">Posted on:  March 27, 2023</p>
              </div>
            </div>
          </div>
          
        </article>
      </section>
  </main>

  <script>
    var script = document.createElement("script");script.src = "https://jk049.github.io/js/initPost.js";
    document.head.appendChild(script);
  </script>

    
    <div class="footer-main ">
  <div class="content-body footer-wraper">
    <div class="footer-box">
      <div class="foot-nav">
        <div class="foot-nav-items">
          <div class="item">
            <div class="logo"></div>
            <div class="email">Email: <a href="mailto:floyd.li@outlook.com">floyd.li@outlook.com</a></div>
          </div>

          <div class="item community">
            <div class="item-title">Social Media</div>
            
              <a href="https://github.com/floyd-li" target="_blank">Github</a>
            
              <a href="https://twitter.com/some-one" target="_blank">Twitter</a>
            
          </div>

          <div class="item resources">
            <div class="item-title">Related</div>
            
              <a href="https://yufengbiji.com/" target="_blank">驭风笔记</a>
            
              <a href="https://apple.com/" target="_blank">Apple</a>
            
          </div>
        </div>
      </div>
      <div class="bottom">
        <div class="item copyright">
          &copy; 2023
          Powered by <a href="https://gohugo.io/" target="_blank">Hugo</a> & <a href="https://github.com/floyd-li/hugo-theme-itheme" target="_blank">iTheme</a>
        </div>
      </div>
    </div>
  </div>
</div>

  </body>
    
    

    
    
</html>
